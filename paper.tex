\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Compute Servers for Teaching Big Data}
\author{Clark Fitzgerald}
\begin{document}

\maketitle

\begin{abstract}

I’ve taught an upper division statistics ``Big Data'' course at UC Davis and CSU Sacramento and have gained some perspective on what works well for this course.
My goals with the course are to get students comfortable with using remote machines and to do realistic analysis of data sets that don’t fit in memory, typically on the order of 100GB or so.
 
I’ve tried what feels like an excessive number of ways to run the server for the students to get the `server experience':
Campus supported Linux cluster (SLURM)
AWS EC2 - student accounts and Jupyter Notebooks
Google colab
NSF Jetstream cloud (Access)
Physical server in the corner of my office (current solution!)

\end{abstract}

The class is called `Big Data'.
For this term to have any meaning, we need to define what we mean by `big'.
For the purposes of this class, a `big' data set is one that is large enough that one cannot load the entire data set into a laptop's memory at once.
Once a data set is this large, it's not possible to simply load the whole thing and start working using R or Python.
My goal is to choose data sets that are comfortably over this threshold, such that the techniques we're learning are motivated, but not gratuitously large such that answering a basic question takes many hours.
Practically speaking, from 2019 until the present I try to choose data sets that are on the order of 10 or 100 GB.


\end{document}
