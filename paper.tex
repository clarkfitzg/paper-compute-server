\documentclass{article}
\usepackage[utf8]{inputenc}

% for \url{}
\usepackage{hyperref}

\title{Compute Servers for Teaching Big Data}
\author{Clark Fitzgerald}
\begin{document}

\maketitle

\begin{abstract}

I've taught an upper division statistics ``Big Data'' course at UC Davis and CSU Sacramento and have gained some perspective on what works well for this course.
My goals with the course are to get students comfortable with using remote machines and to do realistic analysis of data sets that don’t fit in memory, typically on the order of 100GB or so.
 
I’ve tried what feels like an excessive number of ways to run the server for the students to get the `server experience':
Campus supported Linux cluster (SLURM)
AWS EC2 - student accounts and Jupyter Notebooks
Google colab
NSF Jetstream cloud (Access)
Physical server in the corner of my office (current solution!)

\end{abstract}

I developed and have taught the course Stat 129, titled `Analyzing and Processing Big Data'.
The catalog course description is:

\emph{
Statistical analysis of large, complex data sets. Topics include memory efficient data processing, the split-apply-combine strategy, rewriting programs for scalability, handling complex data formats, and applications such as statistical learning, dimension reduction, and efficient data representation. Students will access data and run code on remote servers.
}


Many of the techniques that we teach in this class work equally well on a modern laptop as on a server.

It's essential that students access data and run code on remote servers, because there is a qualitative difference between running code on a laptop that's sitting in front of you and a machine that you never actually see, that you can only access through ssh, secure shell.
I teach bash, the classic command line, because it has proven to be a durable technology; it's been around since the 1970's, and it isn't going away any time soon.
In general, students benefit when instructors focus on proven core technologies, rather than on what happens to be popular this year, because they come away with skills that they can apply in many different contexts.

My goal with the command line is for them to do it enough that they get over the initial fear that comes from moving from a Graphical User Interface (GUI), so they will be more comfortable in the future.
Thus we need an actual remote server that students can log in to so that they can perform the tasks.
The problem is that my job doesn't provide such a server.
If I want students to have this experience, then it is on me to find and manage such a server.
You may be in a similar situation of wanting to teach a class that relies on a server, and are wondering how should you do it?
If so, read on.

\Section{Which Data?}

The class is called `Big Data'.
For this term to have any meaning, we need to define what we mean by `big'.
For the purposes of this class, a `big' data set is one that is large enough that one cannot load the entire data set into a laptop's memory at once.
Once a data set is this large, it's not possible to simply load the whole thing and start working using R or Python.
My goal is to choose data sets that are comfortably over this threshold, such that the techniques we're learning are motivated, but not gratuitously large such that answering a basic question takes many hours.
Practically speaking, from 2019 until the present I try to choose data sets that are on the order of 10 or 100 GB.
The data analysis tasks will typically take 1 to 10 minutes if done in serial, and will be an order of magnitude faster if done in parallel.

While teaching this course, I have to select data sets.
Everything that we do in the course is based on working with a legitimate, unwieldy, data set.
In general, I'm looking for the following characteristics:
\begin{enumerate}
\item \textbf{Publicly Available} data should be available for legal public download.
\item \textbf{Size} large enough to motivate the computational techniques we're learning, but no larger.
\item \textbf{Format} common format, at least one tabular and another nested. Typically we will do a CSV/TSV for the tabular, and XML or JSON for the nested.
\item \textbf{Background Knowledge} a college student should be able to understand what the data actually represents with minimal effort.
\end{enumerate}

Most of these principles I've violated at one time or another, and the difficulties became clear later.

Amazon Open Data \url{https://registry.opendata.aws/} has provided a reliably good source of large data sets fitting these criterion. 
Another good source is the US government \url{https://data.gov/}.

Once I find a data set to work with, I download it to the server and massage it into the form where I want my students to see.
A large part of the course is `cleaning' the data, preparing it for analysis by filtering, imputing, etc.
TODO: cite
However, it requires judgement as to which cleaning tasks are good for the students learning, and which are merely tedious or too uncommon.


\subsection{server options}

I'm an Assistant Professor in the Mathematics and Statistics Department at CSU Sacramento, a regional university.
I have some professional background with computers and programming.
In particular, I've done some server administration, and that experience has definitely proven useful when teaching this class.

Our department plans to offer Stat 129 once a year for the foreseeable future, so we need a long term solution for a server.
In a particular year we'll have one instructor, 15-25 students, and no TA's or other support.
I don't need a large machine for any purpose other than teaching.
Please keep in mind the context I'm coming from when making the following recommendations, as they may not be appropriate for your situation.

What follows are the server options, ordered by my personal preference.

What do you actually need to teach a class like this?

\section{Best option - Dedicated Support and Infrastructure}

If you're in the lucky position of having compute infrastructure with dedicated support in your department, college, or university, then this is an ideal option for teaching classes.
You simply ask the system administrator for what you need, and they'll make it happen.
The only hard part is knowing what it is you need.

While teaching at UC Davis, the system administrator's helped me with the following tasks related to the class:
\begin{itemize}
\item Creating user accounts from a student roster.
\item Installing R, Python, along with various packages and libraries of software.
\item Setting up and configuring an open source database (Postgres) for use by students.
\end{itemize}

A systems administrator can do many other things, of course.
Just ask!

I feel that timely support is much more important the physical compute power, especially if the class is small.

\section{Option 1 - Personal Physical Server}

A personal physical server is either in a rack in a server room somewhere on campus, or else a large box in an office.
If it's in a server rack then you can generally get a machine with higer specs, better internet connectivity, and it's not cluttering up your space.
If the machine is in your office, then when you launch a big job the machine will heat up and the cooling fan will turn on, which provides a very real connection to the amount of data being processed for the students in office hours, which can be fun.

This is my current solution, after having explored and experienced the others described in this article.
I find that it is the most pragmatic solution, requiring the least effort on my part.

The major advantages of this approach are cost and simplicity.
Regarding cost, I've found it easier to fund a one time purchase of a few thousand dollars compared to finding a sponsor who will commit to the ongoing variable subscription fees of the cloud services, even if the total cost would be lower for the cloud subscription.
It's simple because you can set it up one time, and then use the same machine for several years.
I myself am the systems administrator, so I can install whatever I want, whenever I want.
I didn't need any specialized knowledge beyond the basics.
I don't have to worry about any creating and maintaining cloud images.
The downside of this is that I am my own support.
If I spill coffee on your machine, then I'm in trouble.
If something goes wrong, I'm the only one who's going to fix it.

For a class of 15 students, I found that a single 32 core CPU with 128 GB of memory was just sufficient for students to experience the class assignments in the way I intended.
I always recommend that students do their homework well before the due date, because if a handful of users are simultaneously trying to use parallel jobs with all available CPU cores, then they won't see the speedup they are expecting.
In practice it happened occassionaly that students who waited until the last hour to do an assignment didn't get to see their final result, but they weren't surprised, because I had warned them many times and they knew to expect this.
With more students, I believe that students would feel crowded on the server.

One technical detail to be aware of is that you must provide remote access so that others can log on to the machine.
Campus IT handled this for me.
I simply asked them for a hostname, and they took care of it.
Our students need to be either on campus or at home on the campus VPN (Virtual Private Network), and they can login via ssh, for example: \texttt{ssh fitzgerald@nsm-stats.csus.ed}.
I would recommend against setting up a physical server at your home, because this step could be tough depending on your internet provider, and there's no reason you should pay for the electricity instead of campus.

The physical server feels out of date, and lacks the cachet of being `the cloud'.
Despite that, it provides a legitimate and simple way to get students using a remote server.
From a student's perspective, there's no difference between \texttt{ssh} to this machine, or to a virtual machine that you own that's running on the cloud.


\end{document}
